{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "NER or Named Entity Recoginition is a task to recognize names, organization, timestamps or similar info in a text document. \n",
    "\n",
    "* We will model the classifier with Keras as a LSTM with embedding layers.\n",
    "* We will be using a labeled dataset from kaggle.\n",
    "\n",
    "Dataset:\n",
    "https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\n",
    "\n",
    "Keras v2.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"data/ner_dataset.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is structured as a list of sentences, words as elements and their POS tag and NER tag. After the row that contains \"Sentence: n\" in the column named \"Sentece #\", all following rows until a new \"Sentence: n+1\" is found will be part of the sentence n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag\n",
       "0  Sentence: 1      Thousands  NNS      O\n",
       "1          NaN             of   IN      O\n",
       "2          NaN  demonstrators  NNS      O\n",
       "3          NaN           have  VBP      O\n",
       "4          NaN        marched  VBN      O\n",
       "5          NaN        through   IN      O\n",
       "6          NaN         London  NNP  B-geo\n",
       "7          NaN             to   TO      O\n",
       "8          NaN        protest   VB      O\n",
       "9          NaN            the   DT      O"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1048565</th>\n",
       "      <td>NaN</td>\n",
       "      <td>impact</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048566</th>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048567</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>Indian</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048568</th>\n",
       "      <td>NaN</td>\n",
       "      <td>forces</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048569</th>\n",
       "      <td>NaN</td>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>NaN</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>NaN</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>NaN</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence #       Word  POS    Tag\n",
       "1048565              NaN     impact   NN      O\n",
       "1048566              NaN          .    .      O\n",
       "1048567  Sentence: 47959     Indian   JJ  B-gpe\n",
       "1048568              NaN     forces  NNS      O\n",
       "1048569              NaN       said  VBD      O\n",
       "1048570              NaN       they  PRP      O\n",
       "1048571              NaN  responded  VBD      O\n",
       "1048572              NaN         to   TO      O\n",
       "1048573              NaN        the   DT      O\n",
       "1048574              NaN     attack   NN      O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of total words: 1048575\n",
      "Num of different words: 35179\n"
     ]
    }
   ],
   "source": [
    "words = list(set(data[\"Word\"].values))\n",
    "words.append(\"ENDPAD\")\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "\n",
    "print(\"Num of total words: {}\".format(data['Word'].shape[0]))\n",
    "print(\"Num of different words: {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will do now is to build the actual sentences so we can train the model. First we need some preprocessing to fill in the gaps, then we will build a small helper to perform the construction of these sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Sentence #\"] = data[\"Sentence #\"].fillna(method=\"ffill\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s : [(w, p, t) for w, p, t in (zip(s[\"Word\"].values.tolist(),\n",
    "                                                             s[\"POS\"].values.tolist(),\n",
    "                                                             s[\"Tag\"].values.tolist()))]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            self.empty = True\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "sg = SentenceGetter(data)\n",
    "print(sg.get_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47959\n"
     ]
    }
   ],
   "source": [
    "sentences = sg.sentences\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how long sentences are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 104\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAFyBJREFUeJzt3W1MVNnhx/HfAGqlE3AeUANiWnx4YStFM8RdUkVlahrXbvi7xsTGbVy3Ne60NSvZTaAx2xdWyzZlcW0wbLqG7e6rNWYl/+3/hcksFdKSZscFbKOtDxtt3PiAcEd0fFgF7v+F7UQrLAwPM8M9309iwhzOnTmHg/zmnHvvGZdt27YAAMbJSHUDAACpQQAAgKEIAAAwFAEAAIYiAADAUAQAABiKAAAAQxEAAGAoAgAADEUAAIChskaq0NPTo4aGBt28eVMul0vBYFDr16/XkSNH9MknnygnJ0eStGXLFi1fvlySdOzYMbW0tCgjI0MvvfSSSkpKJEldXV1qamrS4OCgKioqVFlZOWIDr1y5klCH/H6/enp6EjpmqqKvzkRfnSmZfc3Pzx9VvREDIDMzUy+++KKKiop07949VVdXq7i4WJL03HPP6fnnn3+i/hdffKH29na99dZbikaj2rt3r95++21J0uHDh7Vnzx75fD7V1NQoEAho3rx5ifYNADABRgwAj8cjj8cjSZo5c6YKCgpkWdaw9SORiMrKyjRt2jTNnj1bc+fO1YULFyRJc+fO1Zw5cyRJZWVlikQiBAAApEhC5wC6u7t18eJFLVy4UJJ0/Phxvfbaazp06JBisZgkybIs+Xy++DFer1eWZT1V7vP5vjJIAACTa8QZwH/cv39fdXV12rZtm7Kzs7Vu3Tpt2rRJkvThhx/q/fffVygU0nC7Sw9V7nK5nioLh8MKh8OSpNraWvn9/tE2UZKUlZWV8DFTFX11JvrqTOnY11EFQH9/v+rq6rRy5UqtWLFCkjRr1qz49ysqKvTmm29KevTOvre3N/49y7Lk9Xol6Yny3t7e+NLS44LBoILBYPxxoidNOKnkTPTVmejr5BjtSeARl4Bs21ZjY6MKCgq0YcOGeHk0Go1//emnn6qwsFCSFAgE1N7erocPH6q7u1tXr17VwoULtWDBAl29elXd3d3q7+9Xe3u7AoFAov0CAEyQEWcAZ8+eVVtbm+bPn6/XX39d0qNLPv/yl7/o0qVLcrlcysvL044dOyRJhYWFevbZZ1VVVaWMjAy9/PLLysh4lDPbt2/Xvn37NDg4qDVr1sRDAwCQfK50/0hI7gMYHn11JvrqTFNyCQgA4EyjvgoI6WPgJ49uvrsuKfP3/zvk9zTE9wDgcQTAFPf4H3wASARLQABgKAIAAAxFAACAoQgAADAUAQAAhiIAAMBQBAAAGIoAAABDEQAAYCgCAAAMRQAAgKHYCyiNjXefHzaGA/BVmAEAgKEIAAAwFEtAhmA5CMB/YwYAAIYiAADAUCwBGYjlIAASMwAAMBYBAACGIgAAwFAEAAAYigAAAEMRAABgKAIAAAxFAACAoQgAADAUAQAAhiIAAMBQ7AWUZsb7KWAAMFrMAADAUCPOAHp6etTQ0KCbN2/K5XIpGAxq/fr1isViqq+v140bN5SXl6fdu3fL7XbLtm01NTWps7NTM2bMUCgUUlFRkSTpxIkT+uijjyRJGzdu1OrVqye1cwCA4Y0YAJmZmXrxxRdVVFSke/fuqbq6WsXFxTpx4oSWLl2qyspKNTc3q7m5WVu3blVnZ6euXbumgwcP6vz583r33Xe1f/9+xWIxHT16VLW1tZKk6upqBQIBud3uSe8kAOBpIy4BeTye+Dv4mTNnqqCgQJZlKRKJqLy8XJJUXl6uSCQiSTp58qRWrVoll8ulxYsX686dO4pGo+rq6lJxcbHcbrfcbreKi4vV1dU1iV0DAHyVhM4BdHd36+LFi1q4cKH6+vrk8XgkPQqJW7duSZIsy5Lf748f4/P5ZFmWLMuSz+eLl3u9XlmWNRF9AACMwaivArp//77q6uq0bds2ZWdnD1vPtu2nylwu15B1hyoPh8MKh8OSpNra2ifCZDSysrISPiadXE/y602Vn9VUH9dE0FdnSse+jioA+vv7VVdXp5UrV2rFihWSpNzcXEWjUXk8HkWjUeXk5Eh69I6/p6cnfmxvb688Ho+8Xq/OnDkTL7csS0uWLHnqtYLBoILBYPzx4881Gn6/P+FjTDZVflYmjSt9daZk9jU/P39U9UZcArJtW42NjSooKNCGDRvi5YFAQK2trZKk1tZWlZaWxsvb2tpk27bOnTun7OxseTwelZSU6NSpU4rFYorFYjp16pRKSkrG0jcAwAQYcQZw9uxZtbW1af78+Xr99dclSVu2bFFlZaXq6+vV0tIiv9+vqqoqSdKyZcvU0dGhXbt2afr06QqFQpIkt9utF154QTU1NZKkTZs2cQVQGuAD4gFzueyhFu3TyJUrVxKqP9WnlKm8EzidA2Cqj2si6KszTcklIACAMxEAAGAoAgAADEUAAICh2A46DbAFNIBUYAYAAIYiAADAUCwBIY6bwgCzMAMAAEMRAABgKAIAAAzFOYAU4dJPAKnGDAAADEUAAIChCAAAMBQBAACGIgAAwFAEAAAYigAAAEMRAABgKAIAAAzFncBJxN2/ANIJMwAAMBQBAACGIgAAwFCcA8CQ+HQwwPmYAQCAoQgAADAUAQAAhiIAAMBQBAAAGIoAAABDEQAAYKgR7wM4dOiQOjo6lJubq7q6OknSkSNH9MknnygnJ0eStGXLFi1fvlySdOzYMbW0tCgjI0MvvfSSSkpKJEldXV1qamrS4OCgKioqVFlZOVl9AgCMwogBsHr1an3/+99XQ0PDE+XPPfecnn/+yc3NvvjiC7W3t+utt95SNBrV3r179fbbb0uSDh8+rD179sjn86mmpkaBQEDz5s2bwK4AABIxYgAsWbJE3d3do3qySCSisrIyTZs2TbNnz9bcuXN14cIFSdLcuXM1Z84cSVJZWZkikQgBAAApNOatII4fP662tjYVFRXpRz/6kdxutyzL0qJFi+J1vF6vLMuSJPl8vni5z+fT+fPnx9HsqYMtoAGkqzEFwLp167Rp0yZJ0ocffqj3339foVBItm0PWX+ocpfLNWTdcDiscDgsSaqtrZXf70+obVlZWQkfM5mup7oBEyAdfp7pNq6Tib46Uzr2dUwBMGvWrPjXFRUVevPNNyU9emff29sb/55lWfJ6vZL0RHlvb688Hs+Qzx0MBhUMBuOPe3p6Emqb3+9P+Bh8tXT4eZo0rvTVmZLZ1/z8/FHVG9NloNFoNP71p59+qsLCQklSIBBQe3u7Hj58qO7ubl29elULFy7UggULdPXqVXV3d6u/v1/t7e0KBAJjeWmkwMBPno//A+AcI84ADhw4oDNnzuj27dvauXOnNm/erNOnT+vSpUtyuVzKy8vTjh07JEmFhYV69tlnVVVVpYyMDL388svKyHiUMdu3b9e+ffs0ODioNWvWxEMDAJAaLnu4hfs0ceXKlYTqp9uU0mnvmlP12QDpNq6Tib46k2OWgAAAUx8BAACGIgAAwFAEAAAYigAAAEMRAABgKAIAAAxFAACAoQgAADAUAQAAhiIAAMBQBAAAGIoAAABDjfkjITE8p+0ACsCZmAEAgKEIAAAwFEtASMjjy1up+nAYABODGQAAGIoAAABDEQAAYCgCAAAMRQAAgKG4CghjxhVBwNTGDAAADMUMYIKw/QOAqYYZAAAYigAAAEMRAABgKAIAAAxFAACAoQgAADAUAQAAhiIAAMBQ3AiGpGHrCCC9jBgAhw4dUkdHh3Jzc1VXVydJisViqq+v140bN5SXl6fdu3fL7XbLtm01NTWps7NTM2bMUCgUUlFRkSTpxIkT+uijjyRJGzdu1OrVqyevVwCAEY24BLR69Wr94he/eKKsublZS5cu1cGDB7V06VI1NzdLkjo7O3Xt2jUdPHhQO3bs0LvvvivpUWAcPXpU+/fv1/79+3X06FHFYrFJ6A4AYLRGDIAlS5bI7XY/URaJRFReXi5JKi8vVyQSkSSdPHlSq1atksvl0uLFi3Xnzh1Fo1F1dXWpuLhYbrdbbrdbxcXF6urqmoTuAABGa0wngfv6+uTxeCRJHo9Ht27dkiRZliW/3x+v5/P5ZFmWLMuSz+eLl3u9XlmWNZ52AwDGaUJPAtu2/VSZy+Uasu5w5eFwWOFwWJJUW1v7RKCMRlZWVsLHjNX1/ylLyutMBaP5mV9PsP7jkjmuqUZfnSkd+zqmAMjNzVU0GpXH41E0GlVOTo6kR+/4e3p64vV6e3vl8Xjk9Xp15syZeLllWVqyZMmQzx0MBhUMBuOPH3++0fD7/Qkfg/FL9GfOuA6PvjpTMvuan58/qnpjWgIKBAJqbW2VJLW2tqq0tDRe3tbWJtu2de7cOWVnZ8vj8aikpESnTp1SLBZTLBbTqVOnVFJSMpaXBgBMkBFnAAcOHNCZM2d0+/Zt7dy5U5s3b1ZlZaXq6+vV0tIiv9+vqqoqSdKyZcvU0dGhXbt2afr06QqFQpIkt9utF154QTU1NZKkTZs2PXViGQCQXCMGwKuvvjpk+RtvvPFUmcvl0o9//OMh669du1Zr165NsHkAgMnCVhAAYCi2gsCEYJsHYOphBgAAhiIAAMBQBAAAGIoAAABDEQAAYCgCAAAMRQAAgKEIAAAwFAEAAIYiAADAUAQAABiKvYAS9PieNwAwlREAmHBsDAdMDSwBAYChCAAAMBQBAACGIgAAwFAEAAAYiquAMKm4bBZIX8wAAMBQBAAAGIoAAABDEQAAYCgCAAAMRQAAgKEIAAAwFAEAAIYiAADAUNwJPArczQrAiZgBAIChCAAAMBQBAACGGtc5gJ/+9Kf62te+poyMDGVmZqq2tlaxWEz19fW6ceOG8vLytHv3brndbtm2raamJnV2dmrGjBkKhUIqKiqaqH4AABI07pPAv/zlL5WTkxN/3NzcrKVLl6qyslLNzc1qbm7W1q1b1dnZqWvXrungwYM6f/683n33Xe3fv3+8Lw8AGKMJXwKKRCIqLy+XJJWXlysSiUiSTp48qVWrVsnlcmnx4sW6c+eOotHoRL88AGCUxj0D2LdvnyTpe9/7noLBoPr6+uTxeCRJHo9Ht27dkiRZliW/3x8/zufzybKseN3/CIfDCofDkqTa2tonjhmNrKyshI8ZyfUJfTZISotxTVf01ZnSsa/jCoC9e/fK6/Wqr69Pv/rVr5Sfnz9sXdu2nypzuVxPlQWDQQWDwfjjnp6ehNrk9/sTPgbJx7gOj746UzL7+lV/ix83riUgr9crScrNzVVpaakuXLig3Nzc+NJONBqNnx/w+XxPdL63t/epd/8AgOQZcwDcv39f9+7di3/9t7/9TfPnz1cgEFBra6skqbW1VaWlpZKkQCCgtrY22batc+fOKTs7mwAAgBQa8xJQX1+ffvvb30qSBgYG9N3vflclJSVasGCB6uvr1dLSIr/fr6qqKknSsmXL1NHRoV27dmn69OkKhUIT0wMAwJi47KEW59PIlStXEqo/Gets7AU08TJ//78J1Wet2Jno6+RIyjkAAMDUxW6gw+BdPwCnYwYAAIYiAADAUAQAABiKAAAAQxEAAGAoAgAADMVloEiJxy+zTfSmMAATgxkAABiKAAAAQ7EEhJRjOQhIDWYAAGAoZgCPYf+f1GM2ACQPMwAAMBQBAACGIgAAwFAEAAAYigAAAEMRAABgKC4DRdp64rLcY+2pawjgUMwAAMBQBAAAGIoAAABDcQ4AU8L1/ymLf80WEcDEYAYAAIYyfgbABnAATGV8AGDqYcdQYGKwBAQAhmIGgCmN2QAwdgQAHOO/z+cQCMBXYwkIAAxl5AyAK3/MMNw4MzMAHkl6AHR1dampqUmDg4OqqKhQZWVlspsAw3HeAHgkqQEwODiow4cPa8+ePfL5fKqpqVEgENC8efOS2QwgjlkCTJbUALhw4YLmzp2rOXPmSJLKysoUiUQIAKQdZgkwQVIDwLIs+Xy++GOfz6fz589P2uux1o+JMFG/R6MJkoGfPK/rCdQHxiOpAWDb9lNlLpfricfhcFjhcFiSVFtbq/z8/IRfJ37M/51MvJFAKhn4OzuW/+NTVbr1NamXgfp8PvX29sYf9/b2yuPxPFEnGAyqtrZWtbW1Y3qN6urqcbVxKqGvzkRfnSkd+5rUAFiwYIGuXr2q7u5u9ff3q729XYFAIJlNAAD8W1KXgDIzM7V9+3bt27dPg4ODWrNmjQoLC5PZBADAvyX9PoDly5dr+fLlk/b8wWBw0p473dBXZ6KvzpSOfXXZQ52ZBQA4HnsBAYChHLMXkJO3mOjp6VFDQ4Nu3rwpl8ulYDCo9evXKxaLqb6+Xjdu3FBeXp52794tt9ud6uZOiMHBQVVXV8vr9aq6ulrd3d06cOCAYrGYvvnNb+rnP/+5srKm/q/vnTt31NjYqMuXL8vlcumVV15Rfn6+I8f1j3/8o1paWuRyuVRYWKhQKKSbN286ZlwPHTqkjo4O5ebmqq6uTpKG/T9q27aamprU2dmpGTNmKBQKqaioKPmNth1gYGDA/tnPfmZfu3bNfvjwof3aa6/Zly9fTnWzJoxlWfbnn39u27Zt37171961a5d9+fJl+4MPPrCPHTtm27ZtHzt2zP7ggw9S2cwJ9fHHH9sHDhywf/3rX9u2bdt1dXX2n//8Z9u2bfudd96xjx8/nsrmTZjf/e53djgctm3bth8+fGjHYjFHjmtvb68dCoXsL7/80rbtR+P5pz/9yVHjevr0afvzzz+3q6qq4mXDjeVnn31m79u3zx4cHLTPnj1r19TUpKTNjlgCenyLiaysrPgWE07h8Xji7w5mzpypgoICWZalSCSi8vJySVJ5eblj+tzb26uOjg5VVFRIenQD4enTp/XMM89IklavXu2Ivt69e1f/+Mc/tHbtWklSVlaWvv71rzt2XAcHB/XgwQMNDAzowYMHmjVrlqPGdcmSJU/N1IYby5MnT2rVqlVyuVxavHix7ty5o2g0mvQ2T8251n9J9hYTqdTd3a2LFy9q4cKF6uvri99I5/F4dOvWrRS3bmK899572rp1q+7duydJun37trKzs5WZmSlJ8nq9siwrlU2cEN3d3crJydGhQ4f0r3/9S0VFRdq2bZsjx9Xr9eoHP/iBXnnlFU2fPl3f+c53VFRU5MhxfdxwY2lZlvx+f7yez+eTZVlP3Rg72RwxA7BHscWEE9y/f191dXXatm2bsrOzU92cSfHZZ58pNzc3NeuhSTYwMKCLFy9q3bp1+s1vfqMZM2aoubk51c2aFLFYTJFIRA0NDXrnnXd0//59dXV1pbpZKZMuf7McMQMYzRYTU11/f7/q6uq0cuVKrVixQpKUm5uraDQqj8ejaDSqnJycFLdy/M6ePauTJ0+qs7NTDx480L179/Tee+/p7t27GhgYUGZmpizLktfrTXVTx83n88nn82nRokWSpGeeeUbNzc2OHNe///3vmj17drwvK1as0NmzZx05ro8bbix9Pp96enri9VL1N8sRMwCnbzFh27YaGxtVUFCgDRs2xMsDgYBaW1slSa2trSotLU1VEyfMD3/4QzU2NqqhoUGvvvqqvv3tb2vXrl361re+pb/+9a+SpBMnTjhifGfNmiWfz6crV65IevRHct68eY4cV7/fr/Pnz+vLL7+UbdvxvjpxXB833FgGAgG1tbXJtm2dO3dO2dnZKQkAx9wI1tHRoT/84Q/xLSY2btyY6iZNmH/+85964403NH/+/Pg0ccuWLVq0aJHq6+vV09Mjv9+vqqoqR1wu+B+nT5/Wxx9/rOrqal2/fv2pywWnTZuW6iaO26VLl9TY2Kj+/n7Nnj1boVBItm07clyPHDmi9vZ2ZWZm6hvf+IZ27twpy7IcM64HDhzQmTNndPv2beXm5mrz5s0qLS0dcixt29bhw4d16tQpTZ8+XaFQSAsWLEh6mx0TAACAxDhiCQgAkDgCAAAMRQAAgKEIAAAwFAEAAIYiAADAUAQAABiKAAAAQ/0/eZlQiWj5ZY8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "sentences_lengs = [len(s) for s in sentences]\n",
    "print(\"Max length: {}\".format(max(sentences_lengs)))\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.hist(sentences_lengs, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras we need padding for sentences. This means that all sentences will have a padding until reaching the max sentence length. We will use a length of 50. This leaves some out of the way but not many.\n",
    "\n",
    "We also need word and tags dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15492\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "\n",
    "print(word2idx['Obama'])\n",
    "print(tag2idx['B-geo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will map these ids to the real sequences, building sequences based on just ids. Both for words and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 50\n",
    "n_words = len(words)\n",
    "n_tags = len(tags)\n",
    "\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words - 1)\n",
    "\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: [('Iranian', 'JJ', 'B-gpe'), ('officials', 'NNS', 'O'), ('say', 'VBP', 'O'), ('they', 'PRP', 'O'), ('expect', 'VBP', 'O'), ('to', 'TO', 'O'), ('get', 'VB', 'O'), ('access', 'NN', 'O'), ('to', 'TO', 'O'), ('sealed', 'JJ', 'O'), ('sensitive', 'JJ', 'O'), ('parts', 'NNS', 'O'), ('of', 'IN', 'O'), ('the', 'DT', 'O'), ('plant', 'NN', 'O'), ('Wednesday', 'NNP', 'B-tim'), (',', ',', 'O'), ('after', 'IN', 'O'), ('an', 'DT', 'O'), ('IAEA', 'NNP', 'B-org'), ('surveillance', 'NN', 'O'), ('system', 'NN', 'O'), ('begins', 'VBZ', 'O'), ('functioning', 'VBG', 'O'), ('.', '.', 'O')]\n",
      "Mapped sentence: [  332  5852  8544  9249 14025 24617 25685 13567 24617  6200  8022 16813\n",
      " 22529 25461 13130  6092 15905 30427 15287 20212 13859  5776  4151 31145\n",
      " 25881 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
      " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
      " 35178 35178]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence: {}\".format(sentences[1]))\n",
    "print(\"Mapped sentence: {}\".format(X[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to conver y to categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there, we need to perform train and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the functional API of Keras to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "\n",
    "input = Input(shape=(max_len,))\n",
    "\n",
    "model = Embedding(input_dim=n_words, output_dim=50, input_length=max_len)(input)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38846 samples, validate on 4317 samples\n",
      "Epoch 1/5\n",
      "38846/38846 [==============================] - 193s 5ms/step - loss: 0.1407 - acc: 0.9647 - val_loss: 0.0630 - val_acc: 0.9819\n",
      "Epoch 2/5\n",
      "38846/38846 [==============================] - 189s 5ms/step - loss: 0.0552 - acc: 0.9839 - val_loss: 0.0511 - val_acc: 0.9851\n",
      "Epoch 3/5\n",
      "38846/38846 [==============================] - 189s 5ms/step - loss: 0.0460 - acc: 0.9866 - val_loss: 0.0490 - val_acc: 0.9856\n",
      "Epoch 4/5\n",
      "38846/38846 [==============================] - 189s 5ms/step - loss: 0.0414 - acc: 0.9879 - val_loss: 0.0456 - val_acc: 0.9864\n",
      "Epoch 5/5\n",
      "38846/38846 [==============================] - 189s 5ms/step - loss: 0.0385 - acc: 0.9886 - val_loss: 0.0453 - val_acc: 0.9863\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=5, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we check some predictions and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            (True ): Pred\n",
      "----            (-    ): -\n",
      "The             (O    ): O\n",
      "U.S.            (B-org): B-org\n",
      "State           (I-org): I-org\n",
      "Department      (I-org): I-org\n",
      "says            (O    ): O\n",
      "the             (O    ): O\n",
      "already         (O    ): O\n",
      "poor            (O    ): O\n",
      "human           (O    ): O\n",
      "rights          (O    ): O\n",
      "situation       (O    ): O\n",
      "in              (O    ): O\n",
      "Nepal           (B-gpe): B-gpe\n",
      "worsened        (O    ): O\n",
      "in              (O    ): O\n",
      "the             (O    ): O\n",
      "past            (B-tim): B-tim\n",
      "year            (O    ): O\n",
      ",               (O    ): O\n",
      "with            (O    ): O\n",
      "security        (O    ): O\n",
      "forces          (O    ): O\n",
      "and             (O    ): O\n",
      "insurgents      (O    ): O\n",
      "alike           (O    ): O\n",
      "committing      (O    ): O\n",
      "serious         (O    ): O\n",
      "human           (O    ): O\n",
      "rights          (O    ): O\n",
      "abuses          (O    ): O\n",
      ".               (O    ): O\n",
      "ENDPAD          (O    ): O\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "i = 500\n",
    "\n",
    "# predicted labels\n",
    "p = model.predict(np.array([X_te[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "\n",
    "# real labels\n",
    "r = np.argmax(y_te[i], axis=-1)\n",
    "\n",
    "print(\"{:15} ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(\"{:15} ({:5}): {}\".format(\"----\", \"-\", \"-\"))\n",
    "for w, real, pred in zip(X_te[i], r, p[0]):\n",
    "    print(\"{:15} ({:5}): {}\".format(words[w], tags[real], tags[pred]))\n",
    "    if words[w] == \"ENDPAD\":\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems we could do better. In fact, we should consider a correct prediction only if all tags are well addressed in the sequence prediction. For now we will just try to improve as this approach **has one big shortcomming: If we haven’t seen a word a prediction time, we have to encode it as unknown and have to infer it’s meaning by it’s surrounding word.**\n",
    "\n",
    "Often word postfix or prefix contains a lot of information about the meaning of the word. Using this information is very important if you deal with texts that contain a lot of rare words and you expect a lot of unknown words at inference time. For example when you work with medical texts.\n",
    "\n",
    "To encode the character-level information, we will use character embeddings and a LSTM to encode every word to a vector. We can use basically everything that produces a single vector for a sequence of characters that represent a word. You can also use a max-pooling architecture or a CNN or whatever works for you. Then we feed the vector to another LSTM together with the learned word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition with character level embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are going to train two models. One for sequences and another one for characters. This means we have to create the dictionaries for words, tags and characters. \n",
    "\n",
    "First we will map the words and tags and also pad the sequences. Notice the increased max_len and the new max_len_char. Also notice that we got a new tag \"UNK\" and \"PAD\" asigned to 1 and 0 values in dictionary respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 75\n",
    "max_len_char = 10\n",
    "\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "tag2idx = {t: i + 1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_word = [[word2idx[w[0]] for w in s] for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the sentences using the \"PAD\" word value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word = pad_sequences(maxlen=max_len, sequences=X_word, value=word2idx[\"PAD\"], padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the dictionary for characters and create a sequence of characters for every possible word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "{'_', 'ë', '?', '°', 'x', 'y', 'Q', '(', '[', 'H', 'W', 'C', 's', 'L', 'U', 'M', '.', '6', '%', 'O', 'o', '\\x96', 'T', 'g', '+', '\\x85', 'P', \"'\", '8', 'i', 'h', '\\x92', 'R', ']', 'V', 'N', 'r', '-', '\\x94', '!', '~', 'I', 'n', 'm', '\\x97', 'G', ':', '#', 'K', '/', '$', 'X', 'v', '7', '`', 'k', ',', '@', 't', 'b', '5', 'ö', 'S', '3', 'j', 'e', '\"', 'F', '\\x91', 'Y', 'J', 'B', 'u', 'd', 'D', 'Z', 'é', 'ü', '9', 'A', 'q', 'E', '1', 'f', '&', '\\x93', 'z', 'a', 'w', ')', ';', '\\xa0', '2', '0', 'p', 'c', '4', 'l'}\n"
     ]
    }
   ],
   "source": [
    "chars = set([w_i for w in words for w_i in w])\n",
    "n_chars = len(chars)\n",
    "print(n_chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx = {c: i + 2 for i, c in enumerate(chars)}\n",
    "char2idx[\"UNK\"] = 1\n",
    "char2idx[\"PAD\"] = 0\n",
    "\n",
    "char2idx[\"Z\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the character model dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_char = []\n",
    "for sentence in sentences:\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"PAD\"))\n",
    "        sent_seq.append(word_seq)\n",
    "    X_char.append(np.array(sent_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping and padding for the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pad_sequences(maxlen=max_len, sequences=y, value=tag2idx[\"PAD\"], padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train and test split for both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_word_tr, X_word_te, y_tr, y_te = train_test_split(X_word, y, test_size=0.1, random_state=2018)\n",
    "X_char_tr, X_char_te, _, _ = train_test_split(X_char, y, test_size=0.1, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the models. \n",
    "\n",
    "We will create embeddings for the characters. Once we got the encodings of characters, we will get the encodings of words based on these character encodings.\n",
    "\n",
    "The input data of the main LSTM will be both the word embedding and character encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout\n",
    "from keras.layers import Bidirectional, concatenate, SpatialDropout1D\n",
    "\n",
    "# input and embedding for words\n",
    "word_in = Input(shape=(max_len,))\n",
    "emb_word = Embedding(input_dim=n_words + 2, output_dim=20,\n",
    "                     input_length=max_len, mask_zero=True)(word_in)\n",
    "\n",
    "# input and embeddings for characters\n",
    "char_in = Input(shape=(max_len, max_len_char,))\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars + 2, output_dim=10,\n",
    "                           input_length=max_len_char, mask_zero=True))(char_in)\n",
    "\n",
    "# character LSTM to get word encodings by characters\n",
    "char_enc = TimeDistributed(LSTM(units=20, return_sequences=False,\n",
    "                                recurrent_dropout=0.5))(emb_char)\n",
    "\n",
    "# main LSTM\n",
    "x = concatenate([emb_word, char_enc])\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "main_lstm = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                               recurrent_dropout=0.6))(x)\n",
    "out = TimeDistributed(Dense(n_tags + 1, activation=\"softmax\"))(main_lstm)\n",
    "\n",
    "model = Model([word_in, char_in], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 75, 10)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 75, 10, 10)   1000        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 75, 20)       703620      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 75, 20)       2480        time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 75, 40)       0           embedding_2[0][0]                \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 75, 40)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 75, 100)      36400       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 75, 18)       1818        bidirectional_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 745,318\n",
      "Trainable params: 745,318\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38846 samples, validate on 4317 samples\n",
      "Epoch 1/10\n",
      "38846/38846 [==============================] - 189s 5ms/step - loss: 0.5533 - acc: 0.8740 - val_loss: 0.2511 - val_acc: 0.9219\n",
      "Epoch 2/10\n",
      "38846/38846 [==============================] - 185s 5ms/step - loss: 0.1993 - acc: 0.9421 - val_loss: 0.1541 - val_acc: 0.9560\n",
      "Epoch 3/10\n",
      "38846/38846 [==============================] - 185s 5ms/step - loss: 0.1357 - acc: 0.9614 - val_loss: 0.1254 - val_acc: 0.9630\n",
      "Epoch 4/10\n",
      "38846/38846 [==============================] - 185s 5ms/step - loss: 0.1122 - acc: 0.9676 - val_loss: 0.1165 - val_acc: 0.9655\n",
      "Epoch 5/10\n",
      "38846/38846 [==============================] - 185s 5ms/step - loss: 0.0996 - acc: 0.9705 - val_loss: 0.1102 - val_acc: 0.9673\n",
      "Epoch 6/10\n",
      "38846/38846 [==============================] - 185s 5ms/step - loss: 0.0913 - acc: 0.9725 - val_loss: 0.1073 - val_acc: 0.9680\n",
      "Epoch 7/10\n",
      "38846/38846 [==============================] - 185s 5ms/step - loss: 0.0852 - acc: 0.9740 - val_loss: 0.1054 - val_acc: 0.9687\n",
      "Epoch 8/10\n",
      "38846/38846 [==============================] - 186s 5ms/step - loss: 0.0807 - acc: 0.9749 - val_loss: 0.1045 - val_acc: 0.9693\n",
      "Epoch 9/10\n",
      "38846/38846 [==============================] - 185s 5ms/step - loss: 0.0766 - acc: 0.9760 - val_loss: 0.1039 - val_acc: 0.9692\n",
      "Epoch 10/10\n",
      "38846/38846 [==============================] - 186s 5ms/step - loss: 0.0733 - acc: 0.9768 - val_loss: 0.1046 - val_acc: 0.9690\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_word_tr,\n",
    "                     np.array(X_char_tr).reshape((len(X_char_tr), max_len, max_len_char))],\n",
    "                    np.array(y_tr).reshape(len(y_tr), max_len, 1),\n",
    "                    batch_size=64, epochs=10, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([X_word_te, \n",
    "                        np.array(X_char_te).reshape((len(X_char_te),\n",
    "                        max_len, max_len_char))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           ||True ||Pred\n",
      "==============================\n",
      "On             : O     O\n",
      "Monday         : B-tim B-tim\n",
      ",              : O     O\n",
      "British        : B-org B-gpe\n",
      "Foreign        : I-org O\n",
      "Secretary      : B-per B-per\n",
      "Jack           : I-per I-per\n",
      "Straw          : I-per I-per\n",
      "said           : O     O\n",
      "his            : O     O\n",
      "government     : O     O\n",
      "has            : O     O\n",
      "found          : O     O\n",
      "no             : O     O\n",
      "evidence       : O     O\n",
      "the            : O     O\n",
      "Bush           : B-org B-per\n",
      "administration : O     O\n",
      "requested      : O     O\n",
      "permission     : O     O\n",
      "to             : O     O\n",
      "fly            : O     O\n",
      "terror         : O     O\n",
      "suspects       : O     O\n",
      "through        : O     O\n",
      "Britain        : B-geo B-geo\n",
      "or             : O     O\n",
      "its            : O     O\n",
      "airspace       : O     O\n",
      ".              : O     O\n"
     ]
    }
   ],
   "source": [
    "i = 1925\n",
    "p = np.argmax(y_pred[i], axis=-1)\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(X_word_te[i], y_te[i], p):\n",
    "    if w != 0:\n",
    "        print(\"{:15}: {:5} {}\".format(idx2word[w], idx2tag[t], idx2tag[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this is without tuning dropout or learning rate. So we could try with those hyperparameters to begin with.\n",
    "\n",
    "Another way to check is to create a new sequence and test it with new data. Let's do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = [\"Hawking\", \"was\", \"a\", \"Fellow\", \"of\", \"the\", \"Royal\", \"Society\", \",\", \"a\", \"lifetime\", \"member\",\n",
    "                 \"of\", \"the\", \"Pontifical\", \"Academy\", \"of\", \"Sciences\", \",\", \"and\", \"a\", \"recipient\", \"of\",\n",
    "                 \"the\", \"Presidential\", \"Medal\", \"of\", \"Freedom\", \",\", \"the\", \"highest\", \"civilian\", \"award\",\n",
    "                 \"in\", \"the\", \"United\", \"States\", \".\"]\n",
    "\n",
    "test_labels = [\"B-per\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-org\", \"I-org\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\",\n",
    "              \"B-org\", \"I-org\", \"I-org\", \"I-org\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\",\n",
    "              \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-geo\", \"I-geo\", \"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "sentences.append(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform every word to it’s integer index. Note that we mapping unknown words to zero. Normally you would want to add a UNKNOWN token to your vocabulary. Then you cut the vocabulary on which you train the model and replace all uncommon words by the UNKNOWN token. We haven’t done this for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences to predict: 1\n",
      "Sentence Max len: 75\n",
      "Word Max len: 10\n",
      "[array([[11, 89, 90, 57, 31, 44, 25,  0,  0,  0],\n",
      "       [90, 89, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [89,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [69, 67, 99, 99, 22, 90,  0,  0,  0,  0],\n",
      "       [22, 85,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [60, 32, 67,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [34, 22,  7, 89, 99,  0,  0,  0,  0,  0],\n",
      "       [64, 22, 97, 31, 67, 60,  7,  0,  0,  0],\n",
      "       [58,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [89,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [99, 31, 85, 67, 60, 31, 45, 67,  0,  0],\n",
      "       [45, 67, 45, 61, 67, 38,  0,  0,  0,  0],\n",
      "       [22, 85,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [60, 32, 67,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [28, 22, 44, 60, 31, 85, 31, 97, 89, 99],\n",
      "       [81, 97, 89, 75, 67, 45,  7,  0,  0,  0],\n",
      "       [22, 85,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [64, 97, 31, 67, 44, 97, 67, 14,  0,  0],\n",
      "       [58,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [89, 44, 75,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [89,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [38, 67, 97, 31, 96, 31, 67, 44, 60,  0],\n",
      "       [22, 85,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [60, 32, 67,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [28, 38, 67, 14, 31, 75, 67, 44, 60, 31],\n",
      "       [17, 67, 75, 89, 99,  0,  0,  0,  0,  0],\n",
      "       [22, 85,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [69, 38, 67, 67, 75, 22, 45,  0,  0,  0],\n",
      "       [58,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [60, 32, 67,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [32, 31, 25, 32, 67, 14, 60,  0,  0,  0],\n",
      "       [97, 31, 54, 31, 99, 31, 89, 44,  0,  0],\n",
      "       [89, 90, 89, 38, 75,  0,  0,  0,  0,  0],\n",
      "       [31, 44,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [60, 32, 67,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [16, 44, 31, 60, 67, 75,  0,  0,  0,  0],\n",
      "       [64, 60, 89, 60, 67, 14,  0,  0,  0,  0],\n",
      "       [18,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(75, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_sent = pad_sequences(sequences=[[word2idx.get(w, 0) for w in test_sentence]],\n",
    "                            padding=\"post\", value=0, maxlen=max_len)\n",
    "\n",
    "print(\"Sentences to predict: {}\".format(len(sentences)))\n",
    "print(\"Sentence Max len: {}\".format(max_len))\n",
    "print(\"Word Max len: {}\".format(max_len_char))\n",
    "\n",
    "x_char = []\n",
    "for sentence in sentences:\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                word_seq.append(char2idx.get(sentence[i][j]))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"PAD\"))\n",
    "        sent_seq.append(word_seq)        \n",
    "    x_char.append(np.array(sent_seq))\n",
    "\n",
    "\n",
    "print(x_char)\n",
    "x_char[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 75)\n",
      "(1, 75, 10)\n",
      "Word           ||True ||Prediction\n",
      "==============================\n",
      "Hawking         B-per O\n",
      "was             O     O\n",
      "a               O     O\n",
      "Fellow          O     O\n",
      "of              O     O\n",
      "the             O     O\n",
      "Royal           B-org B-org\n",
      "Society         I-org I-org\n",
      ",               O     O\n",
      "a               O     O\n",
      "lifetime        O     O\n",
      "member          O     O\n",
      "of              O     O\n",
      "the             O     O\n",
      "Pontifical      B-org B-org\n",
      "Academy         I-org I-org\n",
      "of              I-org I-org\n",
      "Sciences        I-org I-org\n",
      ",               O     O\n",
      "and             O     O\n",
      "a               O     O\n",
      "recipient       O     O\n",
      "of              O     O\n",
      "the             O     O\n",
      "Presidential    O     O\n",
      "Medal           O     B-org\n",
      "of              O     I-org\n",
      "Freedom         O     I-org\n",
      ",               O     O\n",
      "the             O     O\n",
      "highest         O     O\n",
      "civilian        O     O\n",
      "award           O     O\n",
      "in              O     O\n",
      "the             O     O\n",
      "United          B-geo B-geo\n",
      "States          I-geo I-geo\n",
      ".               O     O\n"
     ]
    }
   ],
   "source": [
    "print(x_test_sent.shape)\n",
    "print(np.array(x_char).shape)\n",
    "\n",
    "p = model.predict([x_test_sent, \n",
    "                  np.array(x_char).reshape((len(x_char), max_len, max_len_char))])\n",
    "\n",
    "p = np.argmax(p, axis=-1)\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Prediction\"))\n",
    "print(30 * \"=\")\n",
    "\n",
    "for w, label, pred in zip(test_sentence, test_labels, p[0]):\n",
    "    print(\"{:15} {:5} {}\".format(w, label, idx2tag[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "As commented above, we cannot evaluate our sequences in a tag per tag manner without taking care of several details:\n",
    "* The most common tag will be \"O\" which we don't really care about. This means that **we can manage 70% ish accuracy without really predicting anything correctly but \"O\" tags. Not good.**\n",
    "* It is useful too to see how good we are doing in different tags such as geo, per, org, etc...\n",
    "\n",
    "This mean we will take a look not at accuracy but at precission, recall and f1-score (harmonic mean of precission and recall). The package seqeval does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        geo       1.00      1.00      1.00         1\n",
      "        org       0.67      1.00      0.80         2\n",
      "        per       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.58      0.75      0.65         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "pred_tags = []\n",
    "for pred in p[0]:\n",
    "    pred_tags.append(idx2tag[pred])\n",
    "\n",
    "print(classification_report(test_labels, pred_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
